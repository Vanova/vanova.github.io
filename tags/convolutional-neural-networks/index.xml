<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Convolutional Neural Networks on Ivan Kukanov | Personal Page</title>
    <link>https://vanova.github.io/tags/convolutional-neural-networks/</link>
    <description>Recent content in Convolutional Neural Networks on Ivan Kukanov | Personal Page</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 15 Sep 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://vanova.github.io/tags/convolutional-neural-networks/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>The I2R’s Submission To VOiCES Distance Speaker Recognition Challenge 2019</title>
      <link>https://vanova.github.io/publication/2019/pub2/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://vanova.github.io/publication/2019/pub2/</guid>
      <description>This paper is about the I2R’s submission to the VOiCES from a distance speaker recognition challenge 2019. The submissions were based on the fusion of two x-vectors and two i-vectors subsystems. Main efforts have been focused on the frontend de-reverberation processing, PLDA backend design, score normalization and fusion studies in order to improve the system performance on single channel distant/farfield audio, under noisy conditions. We contribute to the fixed condition task under specific training and development data set.</description>
    </item>
    
    <item>
      <title>I4U Submission to NIST SRE 2018: Leveraging from a Decade of Shared Experiences</title>
      <link>https://vanova.github.io/publication/2019/pub1/</link>
      <pubDate>Tue, 16 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://vanova.github.io/publication/2019/pub1/</guid>
      <description>The I4U consortium was established to facilitate a joint entry to NIST speaker recognition evaluations (SRE). The latest edition of such joint submission was in SRE 2018, in which the I4U submission was among the best-performing systems. SRE&amp;rsquo;18 also marks the 10-year anniversary of I4U consortium into NIST SRE series of evaluation. The primary objective of the current paper is to summarize the results and lessons learned based on the twelve sub-systems and their fusion submitted to SRE&amp;rsquo;18.</description>
    </item>
    
    <item>
      <title>Maximal Figure-of-Merit Embedding for Multi-label Audio Classification</title>
      <link>https://vanova.github.io/publication/2018/pub7/</link>
      <pubDate>Sun, 15 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://vanova.github.io/publication/2018/pub7/</guid>
      <description>This work tackles the problem of the domestic audio tagging or environmental sound classification, where one audio recording can contain one or more acoustic events and a recognizer should output all of those tags. A baseline model for this task is a convolutional recurrent neural network (CRNN) with sigmoid output nodes optimized using the binary crossentropy objective. Traditional error metrics, such as classification error, are not suitable for this type of task.</description>
    </item>
    
    <item>
      <title>Deep learning with Maximal Figure-of-Merit Cost to Advance Multi-label Speech Attribute Detection</title>
      <link>https://vanova.github.io/publication/2016/pub6/</link>
      <pubDate>Mon, 12 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://vanova.github.io/publication/2016/pub6/</guid>
      <description>In this work, we are interested in boosting speech attribute detection by formulating it as a multi-label classification task, and deep neural networks (DNNs) are used to design speech attribute detectors. A straightforward way to tackle the speech attribute detection task is to estimate DNN parameters using the mean squared error (MSE) loss function and employ a sigmoid function in the DNN output nodes. A more principled way is nonetheless to incorporate the micro-F1 measure, which is a widely used metric in the multi-label classification, into the DNN loss function to directly improve the metric of interest at training time.</description>
    </item>
    
    <item>
      <title>Speech attribute detection using deep learning</title>
      <link>https://vanova.github.io/publication/2015/pub1/</link>
      <pubDate>Sat, 01 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://vanova.github.io/publication/2015/pub1/</guid>
      <description>In this work we present alternative models for attribute speech feature extraction based on the two state-of-the-art deep neural networks: convolutional neural networks (CNN) and feed-forward neural network with pretraining using stack of restricted Boltzmann machines (DBN-DNN). These attribute detectors are trained using data-driven approach across all languages in the OGI-TS multi-language telephone speech corpus. Thus, such a detectors can be considered as the universal phonetician alphabet (UPA), which can detect phonologically distinctive articulatory features belonging to all languages.</description>
    </item>
    
  </channel>
</rss>